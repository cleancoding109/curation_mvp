# curation_framework

The 'curation_framework' project was generated by using the default-python template.

For documentation on the Databricks Asset Bundles format use for this project,
and for CI/CD configuration, see https://docs.databricks.com/aws/en/dev-tools/bundles.

## Getting started

Choose how you want to work on this project:

(a) Directly in your Databricks workspace, see
    https://docs.databricks.com/dev-tools/bundles/workspace.

(b) Locally with an IDE like Cursor or VS Code, see
    https://docs.databricks.com/vscode-ext.

(c) With command line tools, see https://docs.databricks.com/dev-tools/cli/databricks-cli.html


Dependencies for this project should be installed using uv:

*  Make sure you have the UV package manager installed.
   It's an alternative to tools like pip: https://docs.astral.sh/uv/getting-started/installation/.
*  Run `uv sync --dev` to install the project's dependencies.

# Using this project using the CLI

The Databricks workspace and IDE extensions provide a graphical interface for working
with this project. It's also possible to interact with it directly using the CLI:

1. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

2. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] curation_framework_job` to your workspace.
    You can find that job by opening your workpace and clicking on **Jobs & Pipelines**.

3. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

   Note that the default job from the template has a schedule that runs every day
   (defined in resources/curation_framework.job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

4. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```

5. Finally, to run tests locally, use `pytest`:
   ```
   $ uv run pytest
   ```

## Recent Updates (Standardization Pipeline)

The pipeline has been refactored to support a robust "Standardization" pattern.

### Key Changes
1.  **Pipeline Architecture**:
    - **Dedup First**: Deduplication now happens *before* transformation to save compute.
    - **Caching**: Source data is cached after reading to prevent re-reading during multiple passes (e.g., merge + watermark).
    - **Safe Watermarking**: Watermarks are updated only *after* a successful merge.
    - **Lazy Evaluation**: Transformations are defined lazily and executed during the write phase.

2.  **Metadata Configuration**:
    - Added `pipeline_type`: Set to `"standardization"` in `claims.json`.
    - Added `watermark_column`: Configured as `_commit_timestamp`.
    - Added `dedup_order_columns`: Configured with `_kafka_offset` as a tiebreaker.

3.  **Job Execution**:
    - `job_executor.py` now accepts a direct `--metadata_path` argument.
    - `claims_daily.yml` is configured as a single-task job pointing to the `claims` metadata.

### Running the Job
To deploy and run the job:
1.  Ensure your Databricks token is valid (`databricks auth login`).
2.  Deploy the bundle: `databricks bundle deploy`.
3.  Run the job: `databricks bundle run claims_daily`.
