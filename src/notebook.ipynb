{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Metadata-Driven Spark Batch Framework - Interactive Notebook\n",
    "\n",
    "This notebook demonstrates how to use the Curation Framework for batch processing from Bronze (Lakeflow Streaming Tables) to Silver (Delta Tables).\n",
    "\n",
    "## Features\n",
    "- **High-Watermark Incremental Processing**: Efficiently process only new records\n",
    "- **SCD Type 1**: Standard Upsert (INSERT/UPDATE)\n",
    "- **SCD Type 2**: History tracking with effective dates\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Bronze Layer (Lakeflow Streaming Tables) \n",
    "    → [Batch Framework with High Watermark] \n",
    "        → Silver Layer (Delta Tables)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configuration path\n",
    "CONFIG_PATH = \"conf/tables_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the framework\n",
    "from curation_framework import (\n",
    "    BatchFrameworkOrchestrator,\n",
    "    SilverProcessor,\n",
    "    process_all_tables,\n",
    "    process_single_table\n",
    ")\n",
    "from curation_framework.utils import (\n",
    "    validate_table_config,\n",
    "    table_exists,\n",
    "    get_table_row_count\n",
    ")\n",
    "\n",
    "print(\"✓ Curation Framework loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Validate Configuration\n",
    "\n",
    "First, let's load the tables configuration and validate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load configuration\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Display tables configuration\n",
    "print(\"Configured Tables:\")\n",
    "print(\"-\" * 60)\n",
    "for table in config.get(\"tables\", []):\n",
    "    status = \"✓ Enabled\" if table.get(\"enabled\", True) else \"✗ Disabled\"\n",
    "    scd_type = f\"SCD{table['scd_type']}\"\n",
    "    print(f\"{status} | {table['table_name']} | {scd_type} | Keys: {table['business_key_columns']}\")\n",
    "\n",
    "print(\"\\nGlobal Settings:\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in config.get(\"global_settings\", {}).items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validate Table Configurations\n",
    "\n",
    "Validate each table configuration to ensure all required fields are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all table configurations\n",
    "all_valid = True\n",
    "for table_config in config.get(\"tables\", []):\n",
    "    errors = validate_table_config(table_config)\n",
    "    if errors:\n",
    "        print(f\"✗ {table_config['table_name']}: {errors}\")\n",
    "        all_valid = False\n",
    "    else:\n",
    "        print(f\"✓ {table_config['table_name']}: Valid\")\n",
    "\n",
    "if all_valid:\n",
    "    print(\"\\n✓ All configurations are valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process All Tables (Full Run)\n",
    "\n",
    "Run the complete batch processing for all enabled tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all enabled tables\n",
    "# Uncomment the line below to run the full batch processing\n",
    "# result = process_all_tables(CONFIG_PATH)\n",
    "\n",
    "# To process specific tables only:\n",
    "# result = process_all_tables(CONFIG_PATH, table_filter=[\"silver_orders\", \"silver_customers\"])\n",
    "\n",
    "print(\"To run batch processing, uncomment the process_all_tables() call above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manual Processing Example\n",
    "\n",
    "This section demonstrates how to use the SilverProcessor class directly for more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual processing with SilverProcessor\n",
    "# This gives you more control over the processing steps\n",
    "\n",
    "# Get the first table config as an example\n",
    "table_config = config[\"tables\"][0]\n",
    "global_settings = config[\"global_settings\"]\n",
    "\n",
    "print(f\"Table: {table_config['table_name']}\")\n",
    "print(f\"Source: {table_config['source_table']}\")\n",
    "print(f\"Target: {table_config['target_table']}\")\n",
    "print(f\"SCD Type: {table_config['scd_type']}\")\n",
    "print(f\"Business Keys: {table_config['business_key_columns']}\")\n",
    "print(f\"Watermark Column: {table_config.get('watermark_column', 'ingestion_ts')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a SilverProcessor instance\n",
    "# processor = SilverProcessor(spark, table_config, global_settings)\n",
    "\n",
    "# Step 1: Get high watermark (last processed timestamp)\n",
    "# watermark = processor.get_high_watermark()\n",
    "# print(f\"High Watermark: {watermark}\")\n",
    "\n",
    "# Step 2: Read incremental data from source\n",
    "# source_df = processor.read_incremental_source(watermark)\n",
    "# print(f\"Records to process: {source_df.count()}\")\n",
    "\n",
    "# Step 3: Apply transformations\n",
    "# transformed_df = processor.apply_transformation(source_df)\n",
    "\n",
    "# Step 4: Process based on SCD type\n",
    "# if processor.scd_type == 1:\n",
    "#     processor.process_scd_type1(transformed_df)\n",
    "# elif processor.scd_type == 2:\n",
    "#     processor.process_scd_type2(transformed_df)\n",
    "\n",
    "print(\"Uncomment the code above to run manual processing steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View SQL Transformation\n",
    "\n",
    "Let's look at a sample SQL transformation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display a sample SQL transformation\n",
    "sql_path = table_config.get(\"transformation_sql_path\", \"conf/sql/orders_transform.sql\")\n",
    "\n",
    "try:\n",
    "    with open(sql_path, 'r') as f:\n",
    "        sql_content = f.read()\n",
    "    print(f\"SQL Transformation: {sql_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(sql_content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"SQL file not found: {sql_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
